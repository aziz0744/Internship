{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412571e9",
   "metadata": {},
   "source": [
    "#                  Web Scrapping assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8702ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open Naukri.com\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field\n",
    "search_field = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='Skills, Designations, Companies']\")\n",
    "search_field.send_keys('Data Scientist')\n",
    "search_field.send_keys(Keys.RETURN)  # Hit Enter to search\n",
    "\n",
    "# Give the page some time to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Apply the location filter (Delhi/NCR) and salary filter (3-6 Lakhs)\n",
    "# For Delhi/NCR\n",
    "location_filter = driver.find_element(By.XPATH, \"//label[contains(text(), 'Delhi / NCR')]\")\n",
    "location_filter.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# For salary (3-6 Lakhs)\n",
    "salary_filter = driver.find_element(By.XPATH, \"//label[contains(text(), '3-6 Lakhs')]\")\n",
    "salary_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Loop through the first 10 job listings\n",
    "job_listings = driver.find_elements(By.CSS_SELECTOR, '.jobTuple')[:10]  # Adjust selector based on actual page structure\n",
    "\n",
    "for job in job_listings:\n",
    "    # Scrape job title\n",
    "    title = job.find_element(By.CSS_SELECTOR, '.title').text\n",
    "    job_titles.append(title)\n",
    "    \n",
    "    # Scrape job location\n",
    "    location = job.find_element(By.CSS_SELECTOR, '.location').text\n",
    "    job_locations.append(location)\n",
    "    \n",
    "    # Scrape company name\n",
    "    company = job.find_element(By.CSS_SELECTOR, '.companyInfo').text\n",
    "    company_names.append(company)\n",
    "    \n",
    "    # Scrape experience required\n",
    "    experience = job.find_element(By.CSS_SELECTOR, '.experience').text\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "jobs_data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbe98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open Shine.com\n",
    "driver.get('https://www.shine.com/')\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the \"Job title, Skills\" field and \"Bangalore\" in the location field\n",
    "job_title_field = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='Job title, Skills']\")\n",
    "job_title_field.send_keys('Data Scientist')\n",
    "\n",
    "location_field = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='Location']\")\n",
    "location_field.send_keys('Bangalore')\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.CSS_SELECTOR, \"button.btn.btn-primary.search-btn\")\n",
    "search_button.click()\n",
    "\n",
    "# Give the page some time to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Let us scrape the first 10 job listings\n",
    "job_listings = driver.find_elements(By.CSS_SELECTOR, '.jobCard')[:10]  # Adjust selector based on actual page structure\n",
    "\n",
    "for job in job_listings:\n",
    "    # Scrape job title\n",
    "    try:\n",
    "        title = job.find_element(By.CSS_SELECTOR, '.job-title').text\n",
    "    except:\n",
    "        title = \"N/A\"\n",
    "    job_titles.append(title)\n",
    "    \n",
    "    # Let us scrape job location\n",
    "    try:\n",
    "        location = job.find_element(By.CSS_SELECTOR, '.location').text\n",
    "    except:\n",
    "        location = \"N/A\"\n",
    "    job_locations.append(location)\n",
    "    \n",
    "    # Let us scrape company name\n",
    "    try:\n",
    "        company = job.find_element(By.CSS_SELECTOR, '.company-name').text\n",
    "    except:\n",
    "        company = \"N/A\"\n",
    "    company_names.append(company)\n",
    "    \n",
    "    # Let us scrape experience required\n",
    "    try:\n",
    "        experience = job.find_element(By.CSS_SELECTOR, '.exp').text\n",
    "    except:\n",
    "        experience = \"N/A\"\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "jobs_data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "\n",
    "# Finally show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open the Flipkart iPhone 11 reviews page\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')\n",
    "\n",
    "# Give the page time to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 2: Let's scrape the first 100 reviews\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "while len(ratings) < 100:\n",
    "    # Find all the review blocks\n",
    "    review_blocks = driver.find_elements(By.CSS_SELECTOR, 'div._1AtVbE')\n",
    "\n",
    "    # Loop through each review block and scrape data\n",
    "    for block in review_blocks:\n",
    "        # Scrape rating\n",
    "        try:\n",
    "            rating = block.find_element(By.CSS_SELECTOR, 'div._3LWZlK').text\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        # Let's scrape review summary\n",
    "        try:\n",
    "            summary = block.find_element(By.CSS_SELECTOR, 'p._2-N8zT').text\n",
    "        except:\n",
    "            summary = \"N/A\"\n",
    "        review_summaries.append(summary)\n",
    "        \n",
    "        # Let's Scrape full review\n",
    "        try:\n",
    "            review = block.find_element(By.CSS_SELECTOR, 'div.t-ZTKy div').text\n",
    "        except:\n",
    "            review = \"N/A\"\n",
    "        full_reviews.append(review)\n",
    "        \n",
    "        # Break out of loop once we have 100 reviews\n",
    "        if len(ratings) >= 100:\n",
    "            break\n",
    "\n",
    "    # Step 3: Now click the 'Next' button to load more reviews (pagination)\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='_1LKTO3' and text()='Next']\")\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Let's wait for the next page to load\n",
    "    except:\n",
    "        print(\"No more pages to load\")\n",
    "        break\n",
    "\n",
    "# Step 4: Create a DataFrame with the scraped data\n",
    "reviews_data = {\n",
    "    'Rating': ratings[:100],\n",
    "    'Review Summary': review_summaries[:100],\n",
    "    'Full Review': full_reviews[:100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(reviews_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5344b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open Flipkart\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "# Step 2: Let's search for \"sneakers\" in the search bar\n",
    "search_field = driver.find_element(By.NAME, 'q')\n",
    "search_field.send_keys('sneakers')\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Let's wait for the search results to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Now scrape the first 100 sneakers\n",
    "brands = []\n",
    "product_descriptions = []\n",
    "prices = []\n",
    "\n",
    "while len(brands) < 100:\n",
    "    # Find all the product blocks on the page\n",
    "    product_blocks = driver.find_elements(By.CSS_SELECTOR, 'a._1fQZEK')\n",
    "\n",
    "    # Loop through each product block and scrape data\n",
    "    for product in product_blocks:\n",
    "        # Scrape brand\n",
    "        try:\n",
    "            brand = product.find_element(By.CSS_SELECTOR, 'div._2WkVRV').text\n",
    "        except:\n",
    "            brand = \"N/A\"\n",
    "        brands.append(brand)\n",
    "        \n",
    "        # Scrape product description\n",
    "        try:\n",
    "            description = product.find_element(By.CSS_SELECTOR, 'a.IRpwTa').text\n",
    "        except:\n",
    "            description = \"N/A\"\n",
    "        product_descriptions.append(description)\n",
    "        \n",
    "        # Scrape price\n",
    "        try:\n",
    "            price = product.find_element(By.CSS_SELECTOR, 'div._30jeq3').text\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "        prices.append(price)\n",
    "\n",
    "        # Break the loop once we have 100 sneakers\n",
    "        if len(brands) >= 100:\n",
    "            break\n",
    "\n",
    "    # Step 4: Click the \"Next\" button to load more sneakers (pagination)\n",
    "    if len(brands) < 100:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[@class='_1LKTO3' and text()='Next']\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except:\n",
    "            print(\"No more pages to load\")\n",
    "            break\n",
    "\n",
    "# Step 5: Create a DataFrame with the scraped data\n",
    "sneakers_data = {\n",
    "    'Brand': brands[:100],\n",
    "    'Product Description': product_descriptions[:100],\n",
    "    'Price': prices[:100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sneakers_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open Amazon India\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 2: Search for \"Laptop\" in the search bar\n",
    "search_field = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search_field.send_keys('Laptop')\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the search results page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Apply the filter for \"Intel Core i7\"\n",
    "# Scroll to CPU Type section and find Intel Core i7 checkbox\n",
    "try:\n",
    "    intel_core_i7_filter = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\")\n",
    "    intel_core_i7_filter.click()\n",
    "    time.sleep(5)  # Wait for filter to apply and page to reload\n",
    "except:\n",
    "    print(\"Intel Core i7 filter not found or could not be clicked.\")\n",
    "\n",
    "# Step 4: Scrape the first 10 laptop listings\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Find all the product blocks on the page (adjust selector as per the page structure)\n",
    "laptop_listings = driver.find_elements(By.CSS_SELECTOR, 'div.s-main-slot div.s-result-item')[:10]\n",
    "\n",
    "for laptop in laptop_listings:\n",
    "    # Let's scrape title\n",
    "    try:\n",
    "        title = laptop.find_element(By.CSS_SELECTOR, 'span.a-size-medium').text\n",
    "    except:\n",
    "        title = \"N/A\"\n",
    "    titles.append(title)\n",
    "    \n",
    "    # Let's scrape ratings\n",
    "    try:\n",
    "        rating = laptop.find_element(By.CSS_SELECTOR, 'span.a-icon-alt').text\n",
    "    except:\n",
    "        rating = \"N/A\"\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Let's scrape price\n",
    "    try:\n",
    "        price = laptop.find_element(By.CSS_SELECTOR, 'span.a-price-whole').text\n",
    "    except:\n",
    "        price = \"N/A\"\n",
    "    prices.append(price)\n",
    "\n",
    "# Step 5: Now create a DataFrame with the scraped data\n",
    "laptops_data = {\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(laptops_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46324043",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15113943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open the AZQuotes website\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 2: Click on the \"Top Quotes\" section\n",
    "top_quotes_link = driver.find_element(By.LINK_TEXT, 'Top Quotes')\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Wait for the top quotes page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Scrape the first 1000 quotes\n",
    "quotes = []\n",
    "authors = []\n",
    "quote_types = []\n",
    "\n",
    "# Continue scraping quotes until we reach 1000\n",
    "while len(quotes) < 1000:\n",
    "    # Find all the quote blocks on the current page\n",
    "    quote_blocks = driver.find_elements(By.CSS_SELECTOR, 'div.mwq')\n",
    "\n",
    "    for block in quote_blocks:\n",
    "        # Scrape the quote text\n",
    "        try:\n",
    "            quote = block.find_element(By.CSS_SELECTOR, 'a.title').text\n",
    "        except:\n",
    "            quote = \"N/A\"\n",
    "        quotes.append(quote)\n",
    "\n",
    "        # Scrape the author\n",
    "        try:\n",
    "            author = block.find_element(By.CSS_SELECTOR, 'div.author').text\n",
    "        except:\n",
    "            author = \"N/A\"\n",
    "        authors.append(author)\n",
    "\n",
    "        # Scrape the type of quote (if available)\n",
    "        try:\n",
    "            quote_type = block.find_element(By.CSS_SELECTOR, 'div.tags a').text\n",
    "        except:\n",
    "            quote_type = \"N/A\"\n",
    "        quote_types.append(quote_type)\n",
    "\n",
    "        # Break if we already have 1000 quotes\n",
    "        if len(quotes) >= 1000:\n",
    "            break\n",
    "\n",
    "    # Step 4: Click on the \"Next\" button to load more quotes (pagination)\n",
    "    if len(quotes) < 1000:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, 'Next')\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except:\n",
    "            print(\"No more pages to load\")\n",
    "            break\n",
    "\n",
    "# Step 5: Create a DataFrame with the scraped data\n",
    "quotes_data = {\n",
    "    'Quote': quotes[:1000],\n",
    "    'Author': authors[:1000],\n",
    "    'Type of Quote': quote_types[:1000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(quotes_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open the JagranJosh webpage\n",
    "driver.get('https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1')\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 2: Scrape the Prime Minister data\n",
    "names = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Find the rows of data in the table or content block\n",
    "pm_rows = driver.find_elements(By.CSS_SELECTOR, 'table tr')\n",
    "\n",
    "for row in pm_rows[1:]:  # Skipping the header row\n",
    "    columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "    \n",
    "    # Scrape name\n",
    "    try:\n",
    "        name = columns[0].text\n",
    "    except:\n",
    "        name = \"N/A\"\n",
    "    names.append(name)\n",
    "    \n",
    "    # Scrape born-dead information\n",
    "    try:\n",
    "        born_died = columns[1].text\n",
    "    except:\n",
    "        born_died = \"N/A\"\n",
    "    born_dead.append(born_died)\n",
    "    \n",
    "    # Scrape term of office\n",
    "    try:\n",
    "        term = columns[2].text\n",
    "    except:\n",
    "        term = \"N/A\"\n",
    "    term_of_office.append(term)\n",
    "    \n",
    "    # Scrape remarks\n",
    "    try:\n",
    "        remark = columns[3].text\n",
    "    except:\n",
    "        remark = \"N/A\"\n",
    "    remarks.append(remark)\n",
    "\n",
    "# Step 3: Create a DataFrame with the scraped data\n",
    "pm_data = {\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': term_of_office,\n",
    "    'Remarks': remarks\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(pm_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# We have to initialize the webdriver (assuming you're using Chrome)\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Step 1: Open the Motor1 website\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 2: Search for '50 most expensive cars' in the search bar\n",
    "search_field = driver.find_element(By.NAME, 'query')  # Search bar element\n",
    "search_field.send_keys('50 most expensive cars')\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the search results to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 3: Click on the correct search result for \"50 most expensive cars in the world\"\n",
    "# Inspecting the search result page, find the link text for the correct article\n",
    "try:\n",
    "    article_link = driver.find_element(By.PARTIAL_LINK_TEXT, '50 Most Expensive Cars in the World')\n",
    "    article_link.click()\n",
    "except:\n",
    "    print(\"Could not find the link for '50 Most Expensive Cars in the World'\")\n",
    "    driver.quit()\n",
    "\n",
    "# Wait for the article page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the car name and price\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# Find all car details blocks (inspect the page to find the correct selector)\n",
    "car_blocks = driver.find_elements(By.CSS_SELECTOR, 'div.listing-item')\n",
    "\n",
    "for block in car_blocks[:50]:  # Limiting to the first 50 cars\n",
    "    # Scrape car name\n",
    "    try:\n",
    "        car_name = block.find_element(By.CSS_SELECTOR, 'h3').text\n",
    "    except:\n",
    "        car_name = \"N/A\"\n",
    "    car_names.append(car_name)\n",
    "    \n",
    "    # Scrape car price\n",
    "    try:\n",
    "        car_price = block.find_element(By.CSS_SELECTOR, 'div.price').text\n",
    "    except:\n",
    "        car_price = \"N/A\"\n",
    "    car_prices.append(car_price)\n",
    "\n",
    "# Step 5: Create a DataFrame with the scraped data\n",
    "cars_data = {\n",
    "    'Car Name': car_names,\n",
    "    'Price': car_prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
